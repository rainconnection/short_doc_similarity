{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e012d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hwan\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import tensor, nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from doc_sim_models import embeding_model, doc_sim_model\n",
    "from doc_to_dataload import preprocess, set_dataLoader\n",
    "from model_process import get_loss\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a0255",
   "metadata": {},
   "source": [
    "### data load\n",
    "* 구조 : \n",
    "* [\n",
    "*  [[문장1], [문장2], ..., [문장n]], # - 1개의 document\n",
    "*  [[문장1], [문장2], ..., [문장n]],\n",
    "*         ...\n",
    "*  [[문장1], [문장2], ..., [문장n]]\n",
    "* ]\n",
    "\n",
    "* sample data를 찾지못해 문장유사도 데이터 활용. 각 document에 문장 1개만 있는 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ffe68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data load\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True) # tokenizer\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_sick(f): \n",
    "\n",
    "    response = requests.get(f).text\n",
    "\n",
    "    lines = response.split(\"\\n\")[1:]\n",
    "    lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "    lines = [l for l in lines if len(l) == 5]\n",
    "    \n",
    "    return [[x[1]] for x in lines], [[x[2]] for x in lines],  np.array([x[3] for x in lines]).astype('float32')\n",
    "    \n",
    "train_1, train_2, train_label = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_train.txt\")\n",
    "valid_1, valid_2, valid_label = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt\")\n",
    "\n",
    "# 길이제한\n",
    "train_1 = train_1[:32]\n",
    "train_2 = train_2[:32]\n",
    "train_label = train_label[:32]\n",
    "valid_1 = valid_1[:32]\n",
    "valid_2 = valid_2[:32]\n",
    "valid_label = valid_label[:32]\n",
    "\n",
    "\n",
    "train_loader = set_dataLoader(*preprocess(train_1, tokenizer),\n",
    "                              *preprocess(train_2, tokenizer),\n",
    "                              tensor(train_label).to(device),\n",
    "                              type = 'train', batch_size = 16)\n",
    "valid_loader = set_dataLoader(*preprocess(valid_1, tokenizer),\n",
    "                              *preprocess(valid_2, tokenizer),\n",
    "                              tensor(valid_label).to(device),\n",
    "                              type = 'train', batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c2824",
   "metadata": {},
   "source": [
    "## model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e838cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_lize :  Linear(in_features=768, out_features=1024, bias=True)\n",
      "initial_lize :  Linear(in_features=1024, out_features=1024, bias=True)\n",
      "initial_lize :  Linear(in_features=1024, out_features=1024, bias=True)\n",
      "initial_lize :  Linear(in_features=1024, out_features=1024, bias=True)\n",
      "initial_lize :  Linear(in_features=1024, out_features=1024, bias=True)\n",
      "initial_lize :  Linear(in_features=1024, out_features=1024, bias=True)\n",
      "initial_lize :  Linear(in_features=1024, out_features=768, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hwan\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "# load bert embedding model\n",
    "bert_emb = embeding_model(DistilBertModel.from_pretrained(\"distilbert-base-uncased\"), pooling_type = 'mean')\n",
    "\n",
    "# 1,2번째 encoding layer 학습 X\n",
    "for param in bert_emb.encoder.transformer.layer[0].parameters():\n",
    "    param.requires_grad = False\n",
    "for param in bert_emb.encoder.transformer.layer[1].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "add_layers =  nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(768,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024,768),\n",
    "            nn.Tanh()\n",
    "        )    \n",
    "\n",
    "initializer = nn.init.xavier_normal_\n",
    "\n",
    "bert_sim_model = doc_sim_model(bert_emb, add_layers = add_layers, initialize = initializer)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "\n",
    "optimizer = AdamW(bert_sim_model.parameters(), lr=1e-5, eps = 1e-16, weight_decay = 0.4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64250b",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c3c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss :  0.7167065143585205 validation loss :  0.5882265865802765  & time :  9.242490291595459  & epoch :  1\n",
      "training loss :  0.7019573152065277 validation loss :  0.6110959053039551  & time :  19.91529893875122  & epoch :  2\n",
      "training loss :  0.671774834394455 validation loss :  0.6226507723331451  & time :  29.13245725631714  & epoch :  3\n",
      "training loss :  0.6572345495223999 validation loss :  0.6298440098762512  & time :  38.68806838989258  & epoch :  4\n"
     ]
    }
   ],
   "source": [
    "# 보통은 함수로 빼서 사용\n",
    "best_loss = 2\n",
    "stopping_count = 0 # for early stopping\n",
    "epochs = 5\n",
    "model = bert_sim_model\n",
    "opt = optimizer\n",
    "sch = scheduler\n",
    "path = './doc_sim_model_' + datetime.now().strftime('%Y_%m_%d') + '.pt'\n",
    "start = time.time() # 시작 시간. 추후 학습시간이 길어지면 멈춤\n",
    "early_stopping = 3\n",
    "for epoch in range(epochs):\n",
    "    ### training\n",
    "    model.train()\n",
    "    losses = []\n",
    "    nums = []\n",
    "    for b in train_loader:\n",
    "        # infer\n",
    "        logits = model(*b[:-1])\n",
    "\n",
    "        # get loss\n",
    "        loss, num = get_loss(logits, b[-1], nn.MSELoss(), opt = opt)\n",
    "        losses.append(loss)\n",
    "        nums.append(num)\n",
    "\n",
    "    train_loss = np.sum(losses)/np.sum(nums)\n",
    "    #torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    ### validation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    nums = []\n",
    "    for b in valid_loader:\n",
    "        # infer\n",
    "        logits = model(*b[:-1])\n",
    "\n",
    "        # get loss\n",
    "        loss, num = get_loss(logits, b[-1], nn.MSELoss())\n",
    "        losses.append(loss)\n",
    "        nums.append(num)\n",
    "\n",
    "    val_loss = np.sum(losses)/np.sum(nums)\n",
    "    #torch.cuda.empty_cache()\n",
    "    if sch is not None:\n",
    "        sch.step(val_loss)\n",
    "    print(\"training loss : \", train_loss, \"validation loss : \", val_loss, \" & time : \", time.time() - start, \" & epoch : \", epoch+1)\n",
    "\n",
    "\n",
    "    ## best model save\n",
    "    if val_loss < best_loss:\n",
    "        torch.save(model, path)\n",
    "        print('best model save at : ' + path)\n",
    "        best_loss = val_loss\n",
    "        stopping_count = 0\n",
    "    else:\n",
    "        stopping_count += 1\n",
    "\n",
    "    ## early_stopping\n",
    "    if stopping_count >= early_stopping:\n",
    "        break\n",
    "    if time.time() - start > 7000:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
